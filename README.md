# ğŸ§  Dataset Longitudinale sulla Moderazione nei Subreddit

Questo progetto di tesi sviluppa un **sistema automatizzato** per la raccolta, archiviazione e parsing di snapshot storici di subreddit pubblici, al fine di costruire un **dataset longitudinale replicabile** per lâ€™analisi retrospettiva della moderazione online.

---

## ğŸ“š Obiettivi

- Raccogliere snapshot HTML semestrali di subreddit dal 2012 al 2024 tramite le API della **Wayback Machine**
- Estrarre informazioni su **moderatori** e **regole** da HTML storici
- Validare e filtrare automaticamente le regole con un **modello ML**
- Archiviare i dati in modo strutturato (MongoDB + cache HTML locale)

---

## ğŸ› ï¸ Tecnologie utilizzate

- Python 3.10+
- MongoDB
- Docker + Docker Compose
- BeautifulSoup, requests, joblib
- XGBoost + Scikit-learn
- Wayback Machine API

---

## ğŸ“¦ Struttura della repository

```
.
â”œâ”€â”€ main.py                     # Scraper principale
â”œâ”€â”€ main_parser.py             # Parser principale
â”œâ”€â”€ scraper/                   # Logica scraping
â”‚   â”œâ”€â”€ scraper.py
â”‚   â”œâ”€â”€ wayback_client.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ logger_setup.py
â”œâ”€â”€ parser/                    # Parsing e ML
â”‚   â”œâ”€â”€ root_parser.py
â”‚   â”œâ”€â”€ parse_homepage.py
â”‚   â”œâ”€â”€ parse_old_homepage.py
â”‚   â”œâ”€â”€ parse_mod.py
â”‚   â”œâ”€â”€ parse_wiki.py
â”‚   â”œâ”€â”€ train_rule_classifier.py
â”‚   â”œâ”€â”€ rule_classifier_xgb.joblib
â”‚   â”œâ”€â”€ rule_vectorizer_xgb.joblib
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ database.py
â”œâ”€â”€ html_cache/                # File HTML archiviati
â”œâ”€â”€ logs/                      # Log di scraping/parsing
â”œâ”€â”€ sample_rules_dataset.tsv  # Dataset etichettato per il training
â”œâ”€â”€ embedding-metadata.tsv    # Lista dei subreddit target
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
```

---

## âš™ï¸ Setup e utilizzo

### ğŸ” Avvio automatico via Docker

```bash
docker-compose up --build
```

Avvia MongoDB e lâ€™intero sistema di scraping e parsing allâ€™interno di un container.



## ğŸ§  Parsing & Classificazione delle Regole

### ğŸ” Riconoscimento Layout
Il modulo `root_parser.py` analizza il tipo di pagina:
- `wiki` â†’ `parse_wiki.py`
- `homepage` â†’ `parse_homepage.py`
- `old homepage` â†’ `parse_old_homepage.py`
- `moderators` â†’ `parse_mod.py`

Ogni parser Ã¨ in grado di gestire layout HTML diversi grazie a funzioni mirate (case 1â€“11).

### ğŸ¤– Classificatore ML

Il file `train_rule_classifier.py` addestra un modello **XGBoost** per distinguere vere regole da contenuti generici.

- Dataset: `sample_rules_dataset.tsv`
- TF-IDF con unigrammi e bigrammi
- Output: `rule_classifier_xgb.joblib` + `rule_vectorizer_xgb.joblib`
- Soglia configurabile (default: `0.105`)

---

## ğŸ“¤ Output

### ğŸ”¸ Collezione `cache` (input)
```json
{
  "_id": "https://web.archive.org/web/202307.../r/funny",
  "file_path": "html_cache/fun/...html",
  "subreddit": "https://www.reddit.com/r/funny/",
  "snapshot_year": 2023,
  "semester": "S1",
  "da_parsare": true
}
```

### ğŸ”¹ Collezione `regole`
```json
{
  "_id": "https://web.archive.org/web/2023.../r/funny",
  "regole": [
    { "title": "No politics" },
    { "title": "Memes go to /r/AdviceAnimals" }
  ]
}
```

### ğŸ”¸ Collezione `moderatori`
```json
{
  "_id": "https://web.archive.org/web/2023.../r/funny",
  "moderatori": [
    { "username": "mod123", "karma": 1240, "timestamp": "2023-07-01T12:00:00Z" }
  ]
}
```

---

## ğŸ“Š Analisi possibile con il dataset

- Evoluzione delle **policy di moderazione**
- Cambiamento nei **moderatori e permessi**
- Analisi **longitudinale** di trend comunitari
- Addestramento di **sistemi automatici di rilevamento regole**

---

## ğŸ”’ Licenza

Questo progetto Ã¨ distribuito con licenza **MIT**.

---

## ğŸ‘¤ Autore

**Leonardo Costantini**  
GitHub: [@leonardoCosta02](https://github.com/leonardoCosta02)  
